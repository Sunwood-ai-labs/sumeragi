#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
LLMå‡¦ç†ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«

LiteLLMã‚’ä½¿ç”¨ã—ãŸLLMã¨ã®å¯¾è©±å‡¦ç†ã‚’ç®¡ç†ã—ã¾ã™ã€‚
"""

import os
import re
from loguru import logger
from litellm import completion
from typing import Optional

class LLMHandler:
    """
    LLMã¨ã®å¯¾è©±ã‚’ç®¡ç†ã™ã‚‹ã‚¯ãƒ©ã‚¹
    """
    def __init__(self, api_key: str, model_name: str):
        """
        åˆæœŸåŒ–

        Args:
            api_key (str): Gemini APIã‚­ãƒ¼
            model_name (str): ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«å
        """
        self.api_key = api_key
        self.model_name = model_name
        self.system_prompt = self._load_system_prompt()
        self.response_pattern = re.compile(r'<llm-response>(.*?)</llm-response>', re.DOTALL)

    def _load_system_prompt(self) -> str:
        """
        ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿ã¾ã™ã€‚

        Returns:
            str: ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        """
        try:
            prompt_path = os.path.join('prompts', 'system.txt')
            if os.path.exists(prompt_path):
                with open(prompt_path, 'r', encoding='utf-8') as f:
                    return f.read().strip()
            else:
                logger.warning('ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“')
                return "ã‚ãªãŸã¯ä¸å¯§ã§è¦ªåˆ‡ãªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚"
        except Exception as e:
            logger.error(f'ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}')
            return "ã‚ãªãŸã¯ä¸å¯§ã§è¦ªåˆ‡ãªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚"

    async def get_response(self, prompt: str) -> str:
        """
        ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«å¯¾ã™ã‚‹LLMã®å¿œç­”ã‚’å–å¾—ã—ã¾ã™ã€‚

        Args:
            prompt (str): ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ

        Returns:
            str: LLMã‹ã‚‰ã®å¿œç­”
        """
        try:
            if not self.api_key:
                return "APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ãªã„ãŸã‚å¿œç­”ã§ãã¾ã›ã‚“ã€‚"

            # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’çµ„ã¿åˆã‚ã›ã‚‹
            messages = [
                {"role": "system", "content": self.system_prompt},
                {"role": "user", "content": prompt}
            ]

            # LiteLLMã‚’ä½¿ç”¨ã—ã¦ãƒ¢ãƒ‡ãƒ«ã«ãƒªã‚¯ã‚¨ã‚¹ãƒˆé€ä¿¡
            response = completion(
                model=self.model_name,
                messages=messages
            )

            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’å–å¾—ã—ã€ãƒ¢ãƒ‡ãƒ«åã‚’è¿½åŠ 
            response_text = response.choices[0].message.content
            
            # <llm-response>ã‚¿ã‚°é–“ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º
            match = self.response_pattern.search(response_text)
            if match:
                extracted_text = match.group(1).strip()
                logger.debug(f"æ­£è¦è¡¨ç¾ã§å¿œç­”ã‚’æŠ½å‡ºã—ã¾ã—ãŸ: {extracted_text[:100]}...")
                return f"{extracted_text}\n\n> ğŸ¤– {self.model_name}"
            else:
                logger.warning("å¿œç­”ã‹ã‚‰<llm-response>ã‚¿ã‚°ã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸ")
                return f"{response_text}\n\n> ğŸ¤– {self.model_name}"
                
        except Exception as e:
            error_msg = f'LLMãƒªã‚¯ã‚¨ã‚¹ãƒˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}'
            logger.error(error_msg)
            return f"å¿œç­”ã®ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"
